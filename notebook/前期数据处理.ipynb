{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['KERAS_BACKEND']='theano'\n",
    "# os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/fchollet/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import numpy as np\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Bidirectional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from pylab import *\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# train数据读取#######################################\n",
    "train_df = pd.read_csv('./data/train_FD001.txt', sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "# train数据读取#######################################\n",
    "\n",
    "# test 数据读取#######################################\n",
    "test_df = pd.read_csv('./data/test_FD001.txt', sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "# test 数据读取#######################################\n",
    "\n",
    "# lable数据读取#######################################\n",
    "truth_df = pd.read_csv('./data/RUL_FD001.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "# lable数据读取#######################################\n",
    "\n",
    "# train数据处理#######################################\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "# 获得各个id下cycle的最大值，shape（100，2）\n",
    "rul.columns = ['id', 'max']\n",
    "# 将rul的列抬头由cycle换为max\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "# 根据id这列，将max这列放到train_df的末尾，对于每个id不同cycle，max的值是一直的\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "# 新增一列RUL，用当前的max减去当前的cycle\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "# 新增cycle_norm这列\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL'])\n",
    "# 找出需要标准化的列\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                             columns=cols_normalize,\n",
    "                             index=train_df.index)\n",
    "# 对需要标准化的列进行标准化\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "# 将没标准化的列和标准化的列合并\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "# 根据train_df.columns的顺序对各列进行重新排序\n",
    "\n",
    "# 将RUL中大于130的值改为130，cycle_norm和RUL无关，所以不管\n",
    "train_df['RUL'].loc[train_df['RUL'] >130]=130\n",
    "# train_df.to_csv('train_df.csv')\n",
    "# train数据处理#######################################\n",
    "\n",
    "# test 数据处理#######################################\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]),\n",
    "                            columns=cols_normalize,\n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "# 将truth_df的RUL一列抬头改为more\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "# 原truth_df的索引为0~99，新增一个id列，其值为1~100\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "# 如第一个零件，test数据中最大运行31个周期，RUL中还有112个周期。故最大周期为143\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "# 将test的最大周期这一列加到test_df中，各id在不同cycle的最大周期一致。\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "# 算得test的实时RUL值\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "# 删掉用来计算RUL的max这一列。\n",
    "# 将RUL中大于130的值改为130\n",
    "# test_df['RUL'].loc[test_df['RUL'] >130]=130\n",
    "# test_df.to_csv('test_df.csv')\n",
    "# test 数据处理#######################################\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "# 将数据格式变为(样本循环次数, 时间窗大小：50, 特征数)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "\n",
    "# 选择特征列\n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# seq_array为用上函数生成的数组，其形状为(15631, 50, 25)\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols))\n",
    "           for id in train_df['id'].unique())\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "\n",
    "# 对应数据格式生成标签\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]\n",
    "\n",
    "# 标签的形状为(15631, 1)\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL'])\n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "\n",
    "# 生成test数据的最后一个序列，形状为(93, 50, 25)，不足100是因为有些测试集小于50\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:]\n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "\n",
    "# 对应生成test的label，形状为(93, 1)\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "# 建模##############################################\n",
    "\n",
    "\n",
    "\n",
    "nb_features = seq_array.shape[2]\n",
    "# nb_features == 25\n",
    "nb_out = label_array.shape[1]\n",
    "# nb_out ==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "412px",
    "left": "401.35px",
    "right": "20px",
    "top": "96px",
    "width": "766px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
